# **OllamaRun**

The OllamaRun Kaggle notebook enables users who do not have the compute resources to run open-source LLMs on their machines to access the GPUs and storage necessary to run these models. You can check out the [Ollama library](https://ollama.com/library) for the list of open-source models that can be pulled and run locally.

Before starting the session, make sure to go to **Settings** > **Accelerator** and select the kind of machine you want to run. In most cases **GPU T4 x2** is powerful enough to run the open-source LLMs locally.
